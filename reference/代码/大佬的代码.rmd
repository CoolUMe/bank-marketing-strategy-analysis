---
title: "EDA and model race in R"
author: "Eryk Walczak"
date: "`r Sys.Date()`"
output:
  html_document:
    number_sections: true
    toc: true
    theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 999) # don't use scientific notation
```

# Overview

This is my old markdown file that I kept on my hard drive for a while. I am releasing it for others to use.

**Source**:

* [S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems](http://repositorium.sdum.uminho.pt/bitstream/1822/30994/1/dss-v3.pdf)

**Problem**:

* binary classification

# Introduction    

Load the packages

```{r, warning = FALSE, message = FALSE}
library(caret) # models
library(corrplot) # correlation plots
library(DALEX) # explain models
library(DescTools) # plots
library(doParallel) # parellel processing
library(dplyr) # syntax
library(ggbiplot) # PCA plots
library(ggplot2) # plots
library(inspectdf) # data overview
library(readr) # quick load
library(sjPlot) # contingency tables
library(tabplot) # data overview
library(tictoc) # measure time
```

## Load the file

```{r}
df <- read_csv("../input/bank-marketing-dataset/bank.csv")
```

Check the file structure

```{r}
glimpse(df)
```


Explore categorical variables

```{r}
inspect_cat(df, show_plot = TRUE)
```

Explore numerical variables

```{r}
inspect_num(df)
```

Check dependent variable - `subscribed`

```{r}
prop.table(table(df$deposit))
```

Dataset is balanced. Resampling is not required.

## Feature engineering

Most clients were not contacted previously

```{r}
table(df$previous)
```

Create new value from `pdays`. -1 means client was not previously contacted

```{r}
df %>%
group_by(pdays) %>%
tally() %>%
arrange(desc(n)) %>%
head()
```

Both values were moderately correlated

```{r}
cor(df$previous, df$pdays)
```

But the similar number of 0 and -1 values in both variables respectively suggests that they describe the same thing.


See how these values are related to opening a deposit

```{r}
ggplot(
    df %>%
    group_by(previous, deposit) %>%
    tally(),
    aes(previous, n, fill = deposit)) +
geom_col() +
theme_bw()
```

Drop the variable `pdays` because it is misleading

```{r}
df$pdays <- NULL
```

Variable `duration` is a leakage and cannot be sensibly used in a predictive model as it is not known before a call is made. It is highly predictive of subscription, i.e. people who spent more time on the call were more likely to subscribe. Initial models which included this variable show that it had the highest importance. [Moro et al. (2014)](https://core.ac.uk/download/pdf/55616194.pdf) also showed that this variable is the most important.

```{r}
ggplot(df, aes(duration, fill = deposit)) +
  geom_density(alpha = 0.5) +
  theme_bw()
```

Remove duration

```{r}
df$duration <- NULL
```

## Overview plot

Identify which variable is the most predictive of a clientâ€™s subscription to the deposit account

```{r}
tableplot(df, sortCol = deposit)
```

It looks like `housing`, `contact`, and  `poutcome` distinguish between both groups.

## Correlations

### Compare numeric variables

```{r}
inspect_cor(df, show_plot = TRUE)
```

Display correlation matrix

```{r}
df_cor <- select_if(df, is.numeric) %>% cor()
corrplot(df_cor, method = "number")
```

## Compare categorical variables

```{r}
# select categorical variables
df_cat <- select_if(df, is.character) %>% names()
# remove the response
response_ind <- match('deposit', df_cat)
df_cat <- df_cat[-response_ind]

# plot categorical variables
for (i in df_cat) {
  print(i)
  
  print(
    sjp.xtab(df$deposit,
         df[[i]],
         margin = "row",
         bar.pos = "stack",
         axis.titles = "deposit",
         legend.title = i)
  )
}
```

Chi-square tests could be used to test distribution across categorical variables

## Dimensionality Reduction 

One way to improve a model is to reduce dimensionality. It can be easily done in caret by altering the `preProcess` argument. I will not use PCA in my models for simplicity. PCA is only used in this kernel for demonstration.

### Principal Component Analysis

```{r}
# keep numeric variables
df_numeric <- select_if(df, is.numeric)

pca <- prcomp(df_numeric, scale = T )

# display components
pca
```

Variance

```{r}
# variance
pr_var <- (pca$sdev)^2 

# % of variance explained
prop_varex <- pr_var/sum(pr_var)

# show percentage of variance of each component
plot(prop_varex,
     xlab = "Principal Component",
     ylab = "Proportion of Variance Explained",
     type = "b" )
```

```{r}
# Scree Plot
plot(cumsum(prop_varex),
     xlab = "Principal Component",
     ylab = "Cumulative Proportion of Variance Explained",
     type = "b" )

# first four components are responsible for around 83% of variance
cumsum(prop_varex)
```

This result means that the majority of information contained in 11 numeric variables can be reduced to 4 principal components.

Here are is a plot of the first two  (explaining most variance):

```{r}
ggbiplot(pca,
         ellipse = TRUE,
         alpha = 0.2,
         groups = df$deposit) +
  theme_bw()
```

Further components can be explored in the same way

```{r}
ggbiplot(pca,
         choices = c(3, 4),
         ellipse = TRUE,
         alpha = 0.2,
         groups = df$deposit) +
  theme_bw()
```

It does not look like using PCA would decrease dimensionality by much. However, adding PCA to [pre-processing](https://topepo.github.io/caret/pre-processing.html) would make models harder to interpret.

# Model1 - use all variables

Split the data into training and test datasets.

```{r}
set.seed(123)
trainIndex <- createDataPartition(df$deposit,
                                  p = 0.8, # training contains 80% of data
                                  list = FALSE)
dfTrain <- df[ trainIndex,]
dfTest  <- df[-trainIndex,]
```

Define model settings

```{r}
set.seed(123)
# splitting
control <- trainControl(method = "cv",
                        number = 10,
                        classProbs = TRUE,
                        summaryFunction = multiClassSummary) # return more metrics than binary classification

# parameter grid for XGBoost
parameterGrid <-  expand.grid(eta = 0.1, # shrinkage (learning rate)
                              colsample_bytree = c(0.5,0.7), # subsample ration of columns
                              max_depth = c(3,6), # max tree depth. model complexity
                              nrounds = 10, # boosting iterations
                              gamma = 1, # minimum loss reduction
                              subsample = 0.8, # ratio of the training instances
                              min_child_weight = 2) # minimum sum of instance weight

# parameter grid for random forest
# mtry = the number of features to use to build each tree
rfGrid <- expand.grid(mtry = seq(from = 4, to = 20, by = 4))
```

Show the grids

```{r}
parameterGrid

rfGrid
```

## Train models

Use parallel processing to speed up training

```{r}
# speed-up the code
# use multiple cores
cl <- makePSOCKcluster(4)
registerDoParallel(cl)
```

### GLM

```{r}
set.seed(123) # for reproducibility
# logistic regression
model_glm <- train(deposit~.,
                   data = dfTrain,
                   method = "glm",
                   family = "binomial",
                   # preProcess = "pca",
                   trControl = control)
print(model_glm)
```

### Random Forest

```{r}
set.seed(123)
# random forest
model_rf <- train(deposit~.,
                  data = dfTrain,
                  method = "rf",
                  ntree = 20,
                  tuneLength = 5,
                  trControl = control,
                  tuneGrid = rfGrid)
print(model_rf)

plot(model_rf)
```

### eXtreme Gradient Boosting

```{r}
set.seed(123)
# XGBoost
# measure execution time
tic()

model_xgb <- train(deposit~.,
                   data = dfTrain,
                   method = "xgbTree",
                   trControl = control,
                   tuneGrid = parameterGrid)
# time
toc()

print(model_xgb)

# plot models
plot(model_xgb)

# best model settings
model_xgb$bestTune
```

Stop the cluster

```{r}
# stop the cluster
stopCluster(cl)
```

## Out-of-sample testing

### Actual predictions

```{r, warning = FALSE}
# Logistic regression
pred_glm_raw <- predict.train(model_glm,
                          newdata = dfTest,
                          type = "raw") # use actual predictions

# Random forest
pred_rf_raw <- predict.train(model_rf,
                          newdata = dfTest,
                          type = "raw")

# XGBoost
pred_xgb_raw <- predict.train(model_xgb,
                          newdata = dfTest,
                          type = "raw")
```

### Probabilities

```{r}
# Logistic regression
pred_glm_prob <- predict.train(model_glm,
                          newdata = dfTest,
                          type = "prob") # use the probabilities

# Random forest
pred_rf_prob <- predict.train(model_rf,
                          newdata = dfTest,
                          type = "prob")

# XGBoost
pred_xgb_prob <- predict.train(model_xgb,
                          newdata = dfTest,
                          type = "prob")
```

### Confusion matrices

```{r}
confusionMatrix(data = pred_glm_raw,
                factor(dfTest$deposit),
                positive = "yes")
confusionMatrix(data = pred_rf_raw,
                factor(dfTest$deposit),
                positive = "yes")
confusionMatrix(data = pred_xgb_raw,
                factor(dfTest$deposit),
                positive = "yes")
```

## Choose the final model

```{r}
model_list <- list(glm = model_glm,
                   rf = model_rf,
                   xgb = model_xgb)

res <- resamples(model_list)

summary(res)
```

### Plot model results - Accuracy, AUC, and F1

Three key performance metrics were chosen to compare model performance.

```{r}
bwplot(res , metric = c("Accuracy", "AUC", "F1"))
```

### Inferential test

Run a t-test to compare model performance

```{r}
compare_models(model_rf, model_xgb)
```

If p-value > 0.5 then the models are not statistically different. P-value is customary and is not [set in stone](https://www.nature.com/articles/s41562-017-0189-z). Other reasons for choosing given model should be considered, e.g. speed, interpretability, portability etc.

## Winner

XGBoost is the winner on Accuracy, but RF performs better on most other metrics. Increasing the number of trees in the RF can drastically improve performance of the RF. 
I kept the number of trees in this script fairly low (20) so that it runs quickly. 
Also, the script was originally developed on a slightly modified version of this dataset where XGBoost was clearly outperforming other models. 
This was the main reason why I decided to continue this analysis with XGBoost, but choosing RF here would not be a bad idea either.

## Explain models

```{r}
p_fun <- function(object, newdata) {
  predict(object,
          newdata = newdata,
          type = "prob")[,2]
  }
yTest <- ifelse(dfTest$deposit == "yes", 1, 0)

# create an explainer
explainer_classif_rf <- DALEX::explain(model_rf, label = "rf",
                                       data = dfTest, y = yTest,
                                       predict_function = p_fun)

explainer_classif_glm <- DALEX::explain(model_glm, label = "glm",
                                       data = dfTest, y = yTest,
                                       predict_function = p_fun)

explainer_classif_xgb <- DALEX::explain(model_xgb, label = "xgb",
                                       data = dfTest, y = yTest,
                                       predict_function = p_fun)

```

## Model performance

Calculate model predictions and residuals

```{r}
mp_classif_rf <- model_performance(explainer_classif_rf)
mp_classif_glm <- model_performance(explainer_classif_glm)
mp_classif_xgb <- model_performance(explainer_classif_xgb)
```

## Plot performance

```{r}
plot(mp_classif_rf, mp_classif_glm, mp_classif_xgb)
```

## Variable importance

Show which variables are the most important in the model.

### Standard method (caret)

```{r}
varImpXGB <- varImp(model_xgb)
varImpXGB
```

### DALEX

```{r, warning = FALSE}
vi_classif_rf <- variable_importance(explainer_classif_rf,
                                     loss_function = loss_root_mean_square)
vi_classif_glm <- variable_importance(explainer_classif_glm,
                                      loss_function = loss_root_mean_square)
vi_classif_xgb <- variable_importance(explainer_classif_xgb,
                                      loss_function = loss_root_mean_square)
plot(vi_classif_rf, vi_classif_glm, vi_classif_xgb)
```

# Develop a model with fewer variables (Model-2) 

Model-2 uses only 5 of the provided variables. This was a subjective decision to decrease the complexity of the model.

## Top 5 variables by importance

The most important variables from Model-1 were chosen.

```{r}
top5 <- varImpXGB$importance %>% head(n = 5)

# display
top5

# assign to a vector to reuse
top5 <- rownames(top5)

# clean condition names to reuse them for plotting and subsetting
top5[match('poutcomesuccess', top5)] <- "poutcome"
top5[match('contactunknown', top5)] <- "contact"
top5[match('housingyes', top5)] <- "housing"
top5[match('loanyes', top5)] <- "loan"
```

## Explore the top variable

```{r}
# describe the variable
Desc(dfTrain[[top5[1]]], plotit = T)
```

## Train the same XGBoost model 

XGBoost model is the same as previously but it uses only top 5 variables.

```{r}
# speed-up the code
# use multiple cores
cl <- makePSOCKcluster(4)
registerDoParallel(cl)

# XGBoost
# define model
top5_model <-
  as.formula(
  paste("deposit", 
        paste(top5, collapse = " + "), 
        sep = " ~ "))
# show the model
top5_model

# train
set.seed(123)
# measure execution time
tic()
model_xgb_top5 <- train(top5_model,
                   data = dfTrain,
                   method = "xgbTree",
                   trControl = control,
                   tuneGrid = parameterGrid)
# time
toc()

print(model_xgb_top5)

# plot models
plot(model_xgb_top5)

# best model settings
model_xgb_top5$bestTune

# stop the cluster
stopCluster(cl)
```

## Predictions

```{r}
pred_xgb_raw_top5 <- predict.train(model_xgb_top5,
                                   newdata = dfTest,
                                   type = "raw")
confusionMatrix(data = pred_xgb_raw_top5,
                factor(dfTest$deposit),
                positive = "yes")
```

# Compare Model-1 and Model-2

Compare two XGBoost models

```{r}
model_list_xgb <- list(xgb_full = model_xgb,
                       xgb_top5 = model_xgb_top5)

res_xgb <- resamples(model_list_xgb)

summary(res_xgb)
```

## Show performance metrics

### Accuracy, AUC and F1

```{r}
bwplot(res_xgb , metric = c("Accuracy", "AUC", "F1"))
```

Full XGBoost model performs better on all metrics.

### Inferential test

```{r}
compare_models(model_xgb, model_xgb_top5)
```

And this difference between the models is statistically significant.

# System settings

Display session settings

```{r}
sessionInfo()
```
